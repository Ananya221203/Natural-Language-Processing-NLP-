{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeNPvcDFF0t1tfxd4NXJn/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya221203/Natural-Language-Processing-NLP-/blob/main/NLP_(for_beginners).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing (NLP)**\n",
        "\n",
        "Objectives:\n",
        "\n",
        "To introduce the basic concepts of NLP.\n",
        "\n",
        "To demonstrate real-world applications of NLP.\n",
        "\n",
        "To engage students with interactive examples and exercises."
      ],
      "metadata": {
        "id": "rhVLBpuqsrdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is NLP?**\n",
        "\n",
        "**Definition:** Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans using natural language.\n",
        "\n",
        "**Simple Explanation:** Think of NLP as teaching computers to understand and talk in human language."
      ],
      "metadata": {
        "id": "-uqcJrfvzHws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Need for NLP**\n",
        "\n",
        "**Understanding Human Language:** Computers need to understand human language to provide meaningful responses.\n",
        "\n",
        "**Automating Repetitive Tasks:** Tasks like sorting emails, summarizing texts, or analyzing sentiments can be automated using NLP.\n",
        "\n",
        "**Enhancing User Experience:** Virtual assistants, chatbots, and translation services all rely on NLP to improve interactions"
      ],
      "metadata": {
        "id": "WOAtYOlEzbnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Applications of NLP**\n",
        "\n",
        "**Machine Translation:** Tools like Google Translate.\n",
        "\n",
        "**Sentiment Analysis:** Understanding emotions in texts (e.g., social media posts).\n",
        "\n",
        "**Text Summarization:** Creating short summaries of long documents.\n",
        "**Speech Recognition:** Voice-activated assistants like Siri or Alexa.\n",
        "Chatbots and Virtual Assistants: Automated customer service bots.**"
      ],
      "metadata": {
        "id": "Lwd8dGUMz2gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**4. Basic steps in NLP**\n",
        "\n",
        "**Text Preprocessing:** Cleaning and preparing text data.\n",
        "\n",
        "**Tokenization:** Splitting text into individual words or phrases.\n",
        "\n",
        "**Removing Stop Words:** Filtering out common words that add little meaning.\n",
        "\n",
        "**Stemming and Lemmatization:** Reducing words to their root forms.\n",
        "\n",
        "**Vectorization:** Converting text into numerical vectors.\n",
        "\n",
        "**Model Building:** Creating machine learning models to analyze text.\n",
        "\n",
        "**Evaluation:** Assessing the performance of the models."
      ],
      "metadata": {
        "id": "wH_DqKDh276g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demonstration Outline:**"
      ],
      "metadata": {
        "id": "zyUr7pDJ3X-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Text Preprocessing**\n"
      ],
      "metadata": {
        "id": "mkPcUWb63dv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAgA_GUMsVqy",
        "outputId": "ce7e9bfa-9cf3-4416-eadd-02a324011820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world i am a univrtsity student  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]|_', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "sample_text = \"Hello World! i am a univrtsity student ___ .\"\n",
        "processed_text = preprocess_text(sample_text)\n",
        "print(processed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tokenisation**"
      ],
      "metadata": {
        "id": "q0d5w6W6q6uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = word_tokenize(processed_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-ZPRnQYsYig",
        "outputId": "026a4e96-206f-4dcb-d5a2-474227e76d01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'i', 'am', 'a', 'univrtsity', 'student']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Removing Stop Words:**"
      ],
      "metadata": {
        "id": "Pb4i2iuEq-oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "NYO35JCUsYkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101d0f7d-7a49-4bf3-9d23-3c329409fe55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'univrtsity', 'student']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words)"
      ],
      "metadata": {
        "id": "qF_7pL_HsYm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6788f5f2-f860-465d-d193-c12833e7c1ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method WordListCorpusReader.words of <WordListCorpusReader in '/root/nltk_data/corpora/stopwords'>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Stemming**"
      ],
      "metadata": {
        "id": "S9zwLHW5rNYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(stemmed_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "8v7pOGarsYqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9b87c6-429f-43eb-9ac4-745f99ecfc16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'univrts', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Lemmatization:**"
      ],
      "metadata": {
        "id": "heEqbVnprSj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "# Download the necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Required for wordnet\n",
        "nltk.download('punkt')  # Ensure punkt is downloaded for tokenization\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Example text\n",
        "text = \"I am very happy.I am going for a trip \"\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example list of filtered tokens\n",
        "filtered_tokens = [\"tree\", \"happy\", \"god\", \"fairly\"]\n",
        "\n",
        "# Check if the required NLTK resources are available\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet.zip')\n",
        "    nltk.data.find('corpora/omw-1.4.zip')\n",
        "    # Lemmatize each token\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    print(\"Lemmatized tokens using NLTK:\", lemmatized_tokens)\n",
        "except LookupError:\n",
        "    print(\"WordNet resource not found. Please ensure it is downloaded properly.\")\n",
        "\n",
        "# Additionally, lemmatize using spaCy\n",
        "doc = nlp(text)\n",
        "spacy_lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatized tokens using spaCy:\", spacy_lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dKSohUPvobj",
        "outputId": "1611e5e2-0959-4c79-dd0a-fb0daf969a67"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens using NLTK: ['tree', 'happy', 'god', 'fairly']\n",
            "Lemmatized tokens using spaCy: ['I', 'be', 'very', 'happy', '.', 'I', 'be', 'go', 'for', 'a', 'trip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Vectorization**"
      ],
      "metadata": {
        "id": "caq0ul2CrbKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "texts = [\"I love drawing.\", \"Python is great.\", \"I hate studies.\", \"Debugging is difficult.\"]\n",
        "labels = [1, 1, 0, 1]  # 1 for positive, 0 for negative\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "vectorized_texts = tfidf_vectorizer.fit_transform(texts)\n",
        "print(vectorized_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB5tpfcMvoe1",
        "outputId": "1c2374d7-76d7-41a1-ea63-5f31428975ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 2)\t0.7071067811865476\n",
            "  (0, 6)\t0.7071067811865476\n",
            "  (1, 3)\t0.6176143709756019\n",
            "  (1, 5)\t0.48693426407352264\n",
            "  (1, 7)\t0.6176143709756019\n",
            "  (2, 8)\t0.7071067811865476\n",
            "  (2, 4)\t0.7071067811865476\n",
            "  (3, 1)\t0.6176143709756019\n",
            "  (3, 0)\t0.6176143709756019\n",
            "  (3, 5)\t0.48693426407352264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Simple Model Building:**"
      ],
      "metadata": {
        "id": "xkjpnh5yrh-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data into training and testing sets.\n",
        "\n",
        "Training a Multinomial Naive Bayes model on the training data.\n",
        "\n",
        "Evaluating the model's performance on the test data using accuracy as a metric.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "akh6h49awUK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(vectorized_texts, labels, test_size=0.25, random_state=42)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkThVNyevohj",
        "outputId": "30aa2d9d-5917-426b-ecf3-cab11f094700"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}